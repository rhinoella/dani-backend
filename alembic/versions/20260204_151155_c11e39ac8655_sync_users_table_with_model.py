"""Sync users table with model

Revision ID: c11e39ac8655
Revises: 008
Create Date: 2026-02-04 15:11:55.878492+00:00

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects import postgresql

# revision identifiers, used by Alembic.
revision: str = 'c11e39ac8655'
down_revision: Union[str, None] = '008'
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    conn = op.get_bind()
    inspector = sa.inspect(conn)

    # Skip infographics table creation if it already exists
    if 'infographics' not in inspector.get_table_names():
        # Create enum types only if they don't exist (idempotent)
        op.execute("""
            DO $$
            BEGIN
                IF NOT EXISTS (SELECT 1 FROM pg_type WHERE typname = 'infographic_style') THEN
                    CREATE TYPE infographic_style AS ENUM ('modern', 'corporate', 'minimal', 'vibrant', 'dark');
                END IF;
            END $$;
        """)

        op.execute("""
            DO $$
            BEGIN
                IF NOT EXISTS (SELECT 1 FROM pg_type WHERE typname = 'infographic_status') THEN
                    CREATE TYPE infographic_status AS ENUM ('pending', 'generating', 'completed', 'failed');
                END IF;
            END $$;
        """)

        op.create_table('infographics',
        sa.Column('id', sa.String(length=36), nullable=False),
        sa.Column('user_id', sa.String(length=36), nullable=True, comment='User who generated the infographic'),
        sa.Column('request', sa.Text(), nullable=False, comment='Original user request for the infographic'),
        sa.Column('topic', sa.String(length=500), nullable=True, comment='Topic used for RAG search'),
        sa.Column('style', postgresql.ENUM('modern', 'corporate', 'minimal', 'vibrant', 'dark', name='infographic_style', create_type=False), nullable=False, comment='Visual style of the infographic'),
    sa.Column('width', sa.Integer(), nullable=False, comment='Image width in pixels'),
    sa.Column('height', sa.Integer(), nullable=False, comment='Image height in pixels'),
    sa.Column('headline', sa.String(length=200), nullable=True, comment='Main headline of the infographic'),
    sa.Column('subtitle', sa.String(length=500), nullable=True, comment='Subtitle or context line'),
    sa.Column('structured_data', postgresql.JSONB(astext_type=sa.Text()), nullable=False, comment='Full structured data (stats, key_points, etc.)'),
    sa.Column('s3_key', sa.String(length=500), nullable=True, comment='S3 object key for the image'),
    sa.Column('s3_bucket', sa.String(length=100), nullable=True, comment='S3 bucket name'),
    sa.Column('image_url', sa.String(length=1000), nullable=True, comment='Direct URL to the image (may be presigned)'),
    sa.Column('image_format', sa.String(length=10), nullable=False, comment='Image format (png, jpg, etc.)'),
    sa.Column('image_size_bytes', sa.Integer(), nullable=True, comment='Image file size in bytes'),
    sa.Column('sources', postgresql.JSONB(astext_type=sa.Text()), nullable=False, comment='RAG sources used (title, date, score)'),
    sa.Column('chunks_used', sa.Integer(), nullable=False, comment='Number of RAG chunks used'),
    sa.Column('confidence_score', sa.Float(), nullable=True, comment='RAG confidence score (0-1)'),
    sa.Column('confidence_level', sa.String(length=20), nullable=True, comment='Confidence level: high, medium, low'),
    sa.Column('retrieval_ms', sa.Float(), nullable=True, comment='Time spent on RAG retrieval'),
    sa.Column('extraction_ms', sa.Float(), nullable=True, comment='Time spent on LLM data extraction'),
    sa.Column('image_gen_ms', sa.Float(), nullable=True, comment='Time spent on image generation'),
    sa.Column('total_ms', sa.Float(), nullable=True, comment='Total generation time'),
    sa.Column('status', postgresql.ENUM('pending', 'generating', 'completed', 'failed', name='infographic_status', create_type=False), nullable=False, comment='Generation status'),
    sa.Column('error_message', sa.Text(), nullable=True, comment='Error message if generation failed'),
    sa.Column('created_at', sa.DateTime(timezone=True), server_default=sa.text('now()'), nullable=False),
    sa.Column('updated_at', sa.DateTime(timezone=True), server_default=sa.text('now()'), nullable=False),
    sa.Column('deleted_at', sa.DateTime(timezone=True), nullable=True, comment='Soft delete timestamp'),
    sa.PrimaryKeyConstraint('id')
        )
        op.create_index(op.f('ix_infographics_s3_key'), 'infographics', ['s3_key'], unique=False)
        op.create_index(op.f('ix_infographics_status'), 'infographics', ['status'], unique=False)
        op.create_index('ix_infographics_status_created', 'infographics', ['status', 'created_at'], unique=False)
        op.create_index('ix_infographics_user_created', 'infographics', ['user_id', 'created_at'], unique=False)
        op.create_index(op.f('ix_infographics_user_id'), 'infographics', ['user_id'], unique=False)
    # End of infographics table creation block

    # Skip conversations table creation if it already exists
    if 'conversations' not in inspector.get_table_names():
        op.create_table('conversations',
    sa.Column('id', sa.String(length=36), nullable=False),
    sa.Column('user_id', sa.String(length=36), nullable=False),
    sa.Column('title', sa.String(length=500), nullable=True, comment='Auto-generated or user-set title'),
    sa.Column('summary', sa.Text(), nullable=True, comment='Auto-generated summary of conversation'),
    sa.Column('metadata', postgresql.JSONB(astext_type=sa.Text()), nullable=False, comment='Output format, filters, settings used'),
    sa.Column('is_archived', sa.Boolean(), nullable=False),
    sa.Column('is_pinned', sa.Boolean(), nullable=False),
    sa.Column('message_count', sa.Integer(), nullable=False),
    sa.Column('created_at', sa.DateTime(timezone=True), server_default=sa.text('now()'), nullable=False),
    sa.Column('updated_at', sa.DateTime(timezone=True), server_default=sa.text('now()'), nullable=False),
    sa.Column('deleted_at', sa.DateTime(timezone=True), nullable=True),
    sa.ForeignKeyConstraint(['user_id'], ['users.id'], ondelete='CASCADE'),
    sa.PrimaryKeyConstraint('id')
        )
        op.create_index(op.f('ix_conversations_user_id'), 'conversations', ['user_id'], unique=False)
        op.create_index('ix_conversations_user_not_deleted', 'conversations', ['user_id', 'deleted_at'], unique=False)
        op.create_index('ix_conversations_user_updated', 'conversations', ['user_id', 'updated_at'], unique=False)

    # Skip messages table creation if it already exists
    if 'messages' not in inspector.get_table_names():
        op.create_table('messages',
    sa.Column('id', sa.String(length=36), nullable=False),
    sa.Column('conversation_id', sa.String(length=36), nullable=False),
    sa.Column('role', sa.String(length=20), nullable=False, comment="'user', 'assistant', or 'system'"),
    sa.Column('content', sa.Text(), nullable=False),
    sa.Column('sources', postgresql.JSONB(astext_type=sa.Text()), nullable=True, comment='RAG sources used for this response'),
    sa.Column('confidence_score', sa.Float(), nullable=True, comment='Confidence score for this response'),
    sa.Column('metadata', postgresql.JSONB(astext_type=sa.Text()), nullable=False, comment='Output format, timings, etc.'),
    sa.Column('tokens_used', sa.Integer(), nullable=True, comment='Estimated tokens used'),
    sa.Column('created_at', sa.DateTime(timezone=True), server_default=sa.text('now()'), nullable=False),
    sa.Column('deleted_at', sa.DateTime(timezone=True), nullable=True),
    sa.ForeignKeyConstraint(['conversation_id'], ['conversations.id'], ondelete='CASCADE'),
    sa.PrimaryKeyConstraint('id')
        )
        op.create_index('ix_messages_conversation_created', 'messages', ['conversation_id', 'created_at'], unique=False)
        op.create_index(op.f('ix_messages_conversation_id'), 'messages', ['conversation_id'], unique=False)
        op.create_index('ix_messages_conversation_not_deleted', 'messages', ['conversation_id', 'deleted_at'], unique=False)

    # Skip rag_logs table creation if it already exists
    if 'rag_logs' not in inspector.get_table_names():
        op.create_table('rag_logs',
    sa.Column('id', sa.UUID(), nullable=False),
    sa.Column('user_id', sa.String(length=36), nullable=True),
    sa.Column('conversation_id', sa.String(length=36), nullable=True),
    sa.Column('query', sa.Text(), nullable=False),
    sa.Column('query_length', sa.Integer(), nullable=False),
    sa.Column('query_intent', sa.String(length=50), nullable=True, comment='Detected intent: factual, comparative, temporal, etc.'),
    sa.Column('query_entities', postgresql.JSONB(astext_type=sa.Text()), nullable=True, comment='Extracted entities from query'),
    sa.Column('chunks_retrieved', sa.Integer(), nullable=False, comment='Number of chunks retrieved'),
    sa.Column('chunks_used', sa.Integer(), nullable=False, comment='Number of chunks actually used in prompt'),
    sa.Column('retrieval_scores', postgresql.JSONB(astext_type=sa.Text()), nullable=True, comment='Similarity scores for retrieved chunks'),
    sa.Column('sources', postgresql.JSONB(astext_type=sa.Text()), nullable=True, comment='Source documents/meetings used'),
    sa.Column('answer_length', sa.Integer(), nullable=True),
    sa.Column('output_format', sa.String(length=50), nullable=True, comment='Requested output format: summary, decisions, etc.'),
    sa.Column('confidence_score', sa.Float(), nullable=True, comment='Overall confidence score (0-1)'),
    sa.Column('confidence_level', sa.String(length=20), nullable=True, comment='high, medium, low, very_low, none'),
    sa.Column('confidence_reason', sa.Text(), nullable=True),
    sa.Column('embedding_latency_ms', sa.Float(), nullable=True),
    sa.Column('retrieval_latency_ms', sa.Float(), nullable=True),
    sa.Column('generation_latency_ms', sa.Float(), nullable=True),
    sa.Column('total_latency_ms', sa.Float(), nullable=True),
    sa.Column('cache_hit', sa.Boolean(), nullable=False),
    sa.Column('cache_type', sa.String(length=20), nullable=True, comment='semantic, exact, embedding, none'),
    sa.Column('success', sa.Boolean(), nullable=False),
    sa.Column('error_type', sa.String(length=100), nullable=True),
    sa.Column('error_message', sa.Text(), nullable=True),
    sa.Column('user_rating', sa.Integer(), nullable=True, comment='1-5 star rating or thumbs up/down (1/0)'),
    sa.Column('user_feedback', sa.Text(), nullable=True, comment='Optional text feedback'),
    sa.Column('feedback_at', sa.DateTime(timezone=True), nullable=True),
    sa.Column('model_used', sa.String(length=100), nullable=True, comment='LLM model used for generation'),
    sa.Column('embedding_model', sa.String(length=100), nullable=True),
    sa.Column('metadata', postgresql.JSONB(astext_type=sa.Text()), nullable=False, comment='Additional metadata, filters used, etc.'),
    sa.Column('created_at', sa.DateTime(timezone=True), server_default=sa.text('now()'), nullable=False),
    sa.ForeignKeyConstraint(['conversation_id'], ['conversations.id'], ondelete='SET NULL'),
    sa.ForeignKeyConstraint(['user_id'], ['users.id'], ondelete='SET NULL'),
    sa.PrimaryKeyConstraint('id')
        )
        op.create_index('ix_rag_logs_cache_hit', 'rag_logs', ['cache_hit'], unique=False)
        op.create_index('ix_rag_logs_confidence_level', 'rag_logs', ['confidence_level'], unique=False)
        op.create_index(op.f('ix_rag_logs_conversation_id'), 'rag_logs', ['conversation_id'], unique=False)
        op.create_index('ix_rag_logs_created_at', 'rag_logs', ['created_at'], unique=False)
        op.create_index('ix_rag_logs_query_intent', 'rag_logs', ['query_intent'], unique=False)
        op.create_index('ix_rag_logs_success', 'rag_logs', ['success'], unique=False)
        op.create_index(op.f('ix_rag_logs_user_id'), 'rag_logs', ['user_id'], unique=False)
    op.alter_column('documents', 'user_id',
               existing_type=sa.VARCHAR(length=36),
               comment='User who uploaded the document',
               existing_nullable=True)
    op.alter_column('documents', 'filename',
               existing_type=sa.VARCHAR(length=500),
               comment='Original filename',
               existing_nullable=False)
    op.alter_column('documents', 'original_filename',
               existing_type=sa.VARCHAR(length=500),
               comment='Original filename uploaded by user',
               existing_nullable=False)
    op.alter_column('documents', 'file_type',
               existing_type=postgresql.ENUM('pdf', 'docx', 'txt', name='document_type'),
               comment='File type (pdf, docx, txt)',
               existing_nullable=False)
    op.alter_column('documents', 'file_size',
               existing_type=sa.BIGINT(),
               comment='File size in bytes',
               existing_nullable=False)
    op.alter_column('documents', 'mime_type',
               existing_type=sa.VARCHAR(length=100),
               comment='MIME type of the file',
               existing_nullable=True)
    op.alter_column('documents', 'storage_key',
               existing_type=sa.VARCHAR(length=1000),
               comment='S3 object key for the original file',
               existing_nullable=True)
    op.alter_column('documents', 'storage_bucket',
               existing_type=sa.VARCHAR(length=100),
               comment='S3 bucket name',
               existing_nullable=True)
    op.alter_column('documents', 'title',
               existing_type=sa.VARCHAR(length=500),
               comment='Document title (extracted or user-provided)',
               existing_nullable=True)
    op.alter_column('documents', 'description',
               existing_type=sa.TEXT(),
               comment='User-provided description',
               existing_nullable=True)
    op.alter_column('documents', 'status',
               existing_type=postgresql.ENUM('pending', 'processing', 'completed', 'failed', name='document_status'),
               comment='Processing status',
               existing_nullable=False,
               existing_server_default=sa.text("'pending'::document_status"))
    op.alter_column('documents', 'error_message',
               existing_type=sa.TEXT(),
               comment='Error message if processing failed',
               existing_nullable=True)
    op.alter_column('documents', 'collection_name',
               existing_type=sa.VARCHAR(length=100),
               comment='Qdrant collection name where chunks are stored',
               existing_nullable=False,
               existing_server_default=sa.text("'documents'::character varying"))
    op.alter_column('documents', 'chunk_count',
               existing_type=sa.INTEGER(),
               comment='Number of chunks created from this document',
               existing_nullable=False,
               existing_server_default=sa.text('0'))
    op.alter_column('documents', 'total_tokens',
               existing_type=sa.INTEGER(),
               comment='Total tokens in the document',
               existing_nullable=False,
               existing_server_default=sa.text('0'))
    op.alter_column('documents', 'metadata',
               existing_type=postgresql.JSONB(astext_type=sa.Text()),
               comment='Additional document metadata (pages, author, etc.)',
               existing_nullable=False,
               existing_server_default=sa.text("'{}'::jsonb"))
    op.alter_column('documents', 'processed_at',
               existing_type=postgresql.TIMESTAMP(timezone=True),
               comment='When processing completed',
               existing_nullable=True)
    # Rename full_name to name
    op.alter_column('users', 'full_name', new_column_name='name')

    # Add metadata with default value for existing rows
    op.add_column('users', sa.Column('metadata', postgresql.JSONB(astext_type=sa.Text()), nullable=True, comment='Additional user metadata'))
    op.execute("UPDATE users SET metadata = '{}'::jsonb WHERE metadata IS NULL")
    op.alter_column('users', 'metadata', nullable=False)

    op.add_column('users', sa.Column('updated_at', sa.DateTime(timezone=True), server_default=sa.text('now()'), nullable=False))

    # Rename last_login to last_login_at
    op.alter_column('users', 'last_login', new_column_name='last_login_at')

    # Add is_active with default True for existing rows
    op.add_column('users', sa.Column('is_active', sa.Boolean(), nullable=True))
    op.execute("UPDATE users SET is_active = true WHERE is_active IS NULL")
    op.alter_column('users', 'is_active', nullable=False)

    op.add_column('users', sa.Column('deleted_at', sa.DateTime(timezone=True), nullable=True))
    op.alter_column('users', 'google_id',
               existing_type=sa.VARCHAR(),
               nullable=False,
               comment="Google's 'sub' claim from ID token")
    op.alter_column('users', 'picture_url',
               existing_type=sa.VARCHAR(),
               type_=sa.Text(),
               comment='Google profile picture URL',
               existing_nullable=True)
    op.alter_column('users', 'created_at',
               existing_type=postgresql.TIMESTAMP(),
               type_=sa.DateTime(timezone=True),
               nullable=False,
               existing_server_default=sa.text('CURRENT_TIMESTAMP'))
    op.drop_constraint(op.f('users_email_key'), 'users', type_='unique')
    op.drop_constraint(op.f('users_google_id_key'), 'users', type_='unique')
    op.create_index(op.f('ix_users_email'), 'users', ['email'], unique=True)
    op.create_index(op.f('ix_users_google_id'), 'users', ['google_id'], unique=True)
    # Columns renamed, not dropped (full_name->name, last_login->last_login_at)
    # ### end Alembic commands ###


def downgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.add_column('users', sa.Column('full_name', sa.VARCHAR(), autoincrement=False, nullable=True))
    op.add_column('users', sa.Column('last_login', postgresql.TIMESTAMP(), autoincrement=False, nullable=True))
    op.drop_index(op.f('ix_users_google_id'), table_name='users')
    op.drop_index(op.f('ix_users_email'), table_name='users')
    op.create_unique_constraint(op.f('users_google_id_key'), 'users', ['google_id'], postgresql_nulls_not_distinct=False)
    op.create_unique_constraint(op.f('users_email_key'), 'users', ['email'], postgresql_nulls_not_distinct=False)
    op.alter_column('users', 'created_at',
               existing_type=sa.DateTime(timezone=True),
               type_=postgresql.TIMESTAMP(),
               nullable=True,
               existing_server_default=sa.text('CURRENT_TIMESTAMP'))
    op.alter_column('users', 'picture_url',
               existing_type=sa.Text(),
               type_=sa.VARCHAR(),
               comment=None,
               existing_comment='Google profile picture URL',
               existing_nullable=True)
    op.alter_column('users', 'google_id',
               existing_type=sa.VARCHAR(),
               nullable=True,
               comment=None,
               existing_comment="Google's 'sub' claim from ID token")
    op.drop_column('users', 'deleted_at')
    op.drop_column('users', 'is_active')
    op.drop_column('users', 'last_login_at')
    op.drop_column('users', 'updated_at')
    op.drop_column('users', 'metadata')
    op.drop_column('users', 'name')
    op.alter_column('documents', 'processed_at',
               existing_type=postgresql.TIMESTAMP(timezone=True),
               comment=None,
               existing_comment='When processing completed',
               existing_nullable=True)
    op.alter_column('documents', 'metadata',
               existing_type=postgresql.JSONB(astext_type=sa.Text()),
               comment=None,
               existing_comment='Additional document metadata (pages, author, etc.)',
               existing_nullable=False,
               existing_server_default=sa.text("'{}'::jsonb"))
    op.alter_column('documents', 'total_tokens',
               existing_type=sa.INTEGER(),
               comment=None,
               existing_comment='Total tokens in the document',
               existing_nullable=False,
               existing_server_default=sa.text('0'))
    op.alter_column('documents', 'chunk_count',
               existing_type=sa.INTEGER(),
               comment=None,
               existing_comment='Number of chunks created from this document',
               existing_nullable=False,
               existing_server_default=sa.text('0'))
    op.alter_column('documents', 'collection_name',
               existing_type=sa.VARCHAR(length=100),
               comment=None,
               existing_comment='Qdrant collection name where chunks are stored',
               existing_nullable=False,
               existing_server_default=sa.text("'documents'::character varying"))
    op.alter_column('documents', 'error_message',
               existing_type=sa.TEXT(),
               comment=None,
               existing_comment='Error message if processing failed',
               existing_nullable=True)
    op.alter_column('documents', 'status',
               existing_type=postgresql.ENUM('pending', 'processing', 'completed', 'failed', name='document_status'),
               comment=None,
               existing_comment='Processing status',
               existing_nullable=False,
               existing_server_default=sa.text("'pending'::document_status"))
    op.alter_column('documents', 'description',
               existing_type=sa.TEXT(),
               comment=None,
               existing_comment='User-provided description',
               existing_nullable=True)
    op.alter_column('documents', 'title',
               existing_type=sa.VARCHAR(length=500),
               comment=None,
               existing_comment='Document title (extracted or user-provided)',
               existing_nullable=True)
    op.alter_column('documents', 'storage_bucket',
               existing_type=sa.VARCHAR(length=100),
               comment=None,
               existing_comment='S3 bucket name',
               existing_nullable=True)
    op.alter_column('documents', 'storage_key',
               existing_type=sa.VARCHAR(length=1000),
               comment=None,
               existing_comment='S3 object key for the original file',
               existing_nullable=True)
    op.alter_column('documents', 'mime_type',
               existing_type=sa.VARCHAR(length=100),
               comment=None,
               existing_comment='MIME type of the file',
               existing_nullable=True)
    op.alter_column('documents', 'file_size',
               existing_type=sa.BIGINT(),
               comment=None,
               existing_comment='File size in bytes',
               existing_nullable=False)
    op.alter_column('documents', 'file_type',
               existing_type=postgresql.ENUM('pdf', 'docx', 'txt', name='document_type'),
               comment=None,
               existing_comment='File type (pdf, docx, txt)',
               existing_nullable=False)
    op.alter_column('documents', 'original_filename',
               existing_type=sa.VARCHAR(length=500),
               comment=None,
               existing_comment='Original filename uploaded by user',
               existing_nullable=False)
    op.alter_column('documents', 'filename',
               existing_type=sa.VARCHAR(length=500),
               comment=None,
               existing_comment='Original filename',
               existing_nullable=False)
    op.alter_column('documents', 'user_id',
               existing_type=sa.VARCHAR(length=36),
               comment=None,
               existing_comment='User who uploaded the document',
               existing_nullable=True)
    op.drop_index(op.f('ix_rag_logs_user_id'), table_name='rag_logs')
    op.drop_index('ix_rag_logs_success', table_name='rag_logs')
    op.drop_index('ix_rag_logs_query_intent', table_name='rag_logs')
    op.drop_index('ix_rag_logs_created_at', table_name='rag_logs')
    op.drop_index(op.f('ix_rag_logs_conversation_id'), table_name='rag_logs')
    op.drop_index('ix_rag_logs_confidence_level', table_name='rag_logs')
    op.drop_index('ix_rag_logs_cache_hit', table_name='rag_logs')
    op.drop_table('rag_logs')
    op.drop_index('ix_messages_conversation_not_deleted', table_name='messages')
    op.drop_index(op.f('ix_messages_conversation_id'), table_name='messages')
    op.drop_index('ix_messages_conversation_created', table_name='messages')
    op.drop_table('messages')
    op.drop_index('ix_conversations_user_updated', table_name='conversations')
    op.drop_index('ix_conversations_user_not_deleted', table_name='conversations')
    op.drop_index(op.f('ix_conversations_user_id'), table_name='conversations')
    op.drop_table('conversations')
    op.drop_index(op.f('ix_infographics_user_id'), table_name='infographics')
    op.drop_index('ix_infographics_user_created', table_name='infographics')
    op.drop_index('ix_infographics_status_created', table_name='infographics')
    op.drop_index(op.f('ix_infographics_status'), table_name='infographics')
    op.drop_index(op.f('ix_infographics_s3_key'), table_name='infographics')
    op.drop_table('infographics')
    # ### end Alembic commands ###
